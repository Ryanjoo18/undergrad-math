\chapter{Partial Derivatives}
In multivariable calculus, we extend notions of differential calculus from functions of one variable to more general functions
\[ f:\RR^n\to\RR^m. \]

\section{Computation of partial derivatives}
Let $f:\RR^n\to\RR$ be a function of $n$ variables $x_1,x_2,\dots,x_n$. Then the \vocab{partial derivative}
\[ \pdv{f}{x_i}(p_1,\dots,p_n) \]
is the rate of change of $f$, at $(p_1,\dots,p_n)$, when we vary only the variable $x_i$ about $p_i$ and keep all of the other variables constant. Precisely, we have
\begin{equation}\label{eqn:partial-diff}
\pdv{f}{x_i}(p_1,\dots,p_n)=\lim_{h\to0}\frac{f(p_1,\dots,p_{i-1},p_i+h,p_{i+1},\dots,p_n)-f(p_1,\dots,p_n)}{h}.
\end{equation}

\begin{notation}
We shall occasionally write $f_x$ for $\delta f/\delta x$, etc.
\end{notation}

Derivatives such as \cref{eqn:partial-diff}, where $f$ has been differentiated once, are called \vocab{first order} partial derivatives. We will look at higher orders in a moment.

\begin{exercise}
Find all the first order derivatives of
\[ f(x,y,z)=x^2+ye^{2x}+\frac{z}{y}. \]
\end{exercise}

\begin{solution}
Keep in mind that we only need to find the derivative of functions with respect to one variable by keeping the rest of the variables constant.

Thus we have
\[ \pdv{f}{x}=2x+2ye^{2x}, \quad \pdv{f}{y}=e^{2x}-\frac{z}{y^2}, \quad \pdv{f}{z}=\frac{1}{y}. \]
\end{solution}

We define second and higher order partial derivatives in a similar manner to how we define them for full derivatives. So, in the case of second order partial derivatives of a function $f(x,y)$ we have
\[ \begin{split}
f_{xx} &= \pdv{}{x}\brac{\pdv{f}{x}} = \pdv[order={2}]{f}{x} \\
f_{yy} &= \pdv{}{y}\brac{\pdv{f}{y}} = \pdv[order={2}]{f}{y} \\
f_{xy} &= \pdv{}{y}\brac{\pdv{f}{x}} = \pdv{f}{x,y} \\
f_{yx} &= \pdv{}{x}\brac{\pdv{f}{y}} = \pdv{f}{y,x}
\end{split} \]

Observe that
\[ \pdv{f}{y,x}=\pdv{f}{x,y}, \quad \pdv{f}{z,x}=\pdv{f}{x,z}, \quad \pdv{f}{z,y}=\pdv{f}{y,z}. \]
This will typically be the case in the examples we will see in this course, but it is not guaranteed unless the derivatives in question are continuous.

we might have a function $f(u,v)$ of two variables $u$ and $v$, each of which is itself a function of $x$ and $y$. We can make the composition
\[ F(x,y)=f(u(x,y),v(x,y)) \]
which is a function of $x$ and $y$, and we might then want to calculate the partial derivatives
\[ \pdv{F}{x}\text{ and }\pdv{F}{y}. \]

\begin{theorem}[Chain rule]
Let $F(t)=f(u(t),v(t))$ with $u$ and $v$ differentiable and $f$ being continuously differentiable in each variable. Then
\begin{equation}
\dv{F}{t}=\pdv{f}{u}\pdv{u}{t}+\pdv{f}{v}\pdv{v}{t}
\end{equation}
\end{theorem}

\begin{corollary}
Let $F(x,y)=f(u(x,y),v(x,y))$ with $u$ and $v$ differentiable in each variable and $f$ being continuously differentiable in each variable. Then
\[ \pdv{F}{x}=\pdv{f}{u}\pdv{u}{x}+\pdv{f}{v}\pdv{v}{x}, \quad \pdv{F}{y}=\pdv{f}{u}\pdv{u}{y}+\pdv{f}{v}\pdv{v}{y}. \]
\end{corollary}

\begin{exercise}
A particle $P$ moves in three dimensional space on a helix so that at time $t$,
\[ x(t)=\cos t, \quad y(t)=\sin t, \quad z(t)=t. \]
The temperature $T$ at $(x,y,z)$ equals $xy+yz+zx$. Use the chain rule to calculate $\dv{T}{t}$.
\end{exercise}

\begin{solution}
The chain rule in this case says that
\begin{align*}
\dv{T}{t} &= \pdv{T}{x}\dv{x}{t}+\pdv{T}{y}\dv{y}{t}+\pdv{T}{z}\dv{z}{t} \\
&= (y+z)(-\sin t)+(x+z)\cos t+(y+x)(1) \\
&= (\sin t+t)(-\sin t)+(\cos t+t)\cos t+(\cos t+\sin t) \\
&= -\sin^2t+\cos^2t+\sin t+\cos t+t\cos t-t\sin t.
\end{align*}
\end{solution}

\begin{exercise}
Let $z=f(x,y)$, where $f$ is an arbitrary differentiable function in one variable. Show that
\[ x\pdv{z}{x}-y\pdv{z}{y}=0. \]
\end{exercise}

\begin{solution}
By the chain rule,
\[ \pdv{z}{x}=yf^\prime(xy) \quad \text{and} \quad \pdv{z}{y}=xf^\prime(xy), \]
where the prime denotes the derivative with respect to $xy$. Hence we have
\[ x\pdv{z}{x}-y\pdv{z}{y}=xyf^\prime(xy)-yxf^\prime(xy)=0. \]
\end{solution}
\pagebreak

\section{Coordinate Systems and Jacobians}
Suppose we have $x=x(u,v)$ and $y=y(u,v)$ and a transformation of coordinates, or a change of variables, given by the mapping $(x,y)\mapsto(u,v)$. We only consider those transformations which have continuous partial derivatives. By chain rule, if $f(x,y)$ is a function with continuous partial derivatives then we can write
\begin{equation}
\begin{pmatrix}
\pdv{f}{u}&\pdv{f}{v}
\end{pmatrix}=
\begin{pmatrix}
\pdv{f}{x}&\pdv{f}{y}
\end{pmatrix}
\begin{pmatrix}
\pdv{x}{u}&\pdv{x}{v}\\
\pdv{y}{u}&\pdv{y}{v}
\end{pmatrix}.
\end{equation}

The matrix
\[\begin{pmatrix}
\pdv{x}{u}&\pdv{x}{v}\\
\pdv{y}{u}&\pdv{y}{v}
\end{pmatrix}\]
is called the \vocab{Jacobian matrix}. Its determinant, denoted by $\dfrac{\partial(x,y)}{\partial(u,v)}$, is called the \vocab{Jacobian}, so that
\[\frac{\partial(x,y)}{\partial(u,v)}=\begin{vmatrix}
\pdv{x}{u}&\pdv{x}{v}\\
\pdv{y}{u}&\pdv{y}{v}
\end{vmatrix}=\pdv{x}{u}\pdv{y}{v}-\pdv{x}{v}\pdv{y}{u}.\]
The Jacobian, or rather its modulus, is a measure of how a map stretches space locally, near a particular point, when this stretching effect varies from point to point. That is to say, under the transformation $(x,y)\mapsto(u,v)$, the area element $\dd{u}\dd{y}$ in the $xy$-plane is equivalent to $\absolute{\frac{\partial(x,y)}{\partial(u,v)}}\dd{u}\dd{v}$, where $\dd{u}\dd{v}$ is the area element in $uv$-plane. If $A$ is a domain in the $xy$-plane mapped one to one and onto a domain $B$ in the $uv$-plane then
\[\int_Af(x,y)\dd{x}\dd{y}=\int_Bf\brac{x(u,v),y(u,v)}\absolute{\frac{\partial(x,y)}{\partial(u,v)}}\dd{u}\dd{v}.\]

\subsection{Plane polar coordinates}
If $P=(x,y)\in\RR^2$ and $(x,y)\neq(0,0)$ then we can determine the position of $(x,y)$ by its distance $r$ from $(0,0)$ and the anti-clockwise angle $\theta$ that $\overrightarrow{OP}$ makes with the $x$-axis. That is,
\begin{equation}
x=r\cos\theta,\quad y=r\sin\theta.
\end{equation}
Note that $r\in[0,\infty)$ and $\theta\in[0,2\pi)$, or equally $(-\pi,\pi]$. Note also that $\theta$ is undefined at the origin.

The Jacobian matrix for this transformation is
\[\begin{pmatrix}
\pdv{x}{r}&\pdv{x}{\theta}\\
\pdv{y}{r}&\pdv{y}{\theta}
\end{pmatrix}=
\begin{pmatrix}
\cos\theta&-r\sin\theta\\
\sin\theta&r\cos\theta
\end{pmatrix}\]
and its Jacobian is
\[\frac{\partial(x,y)}{\partial(r,\theta)}=\begin{vmatrix}
\cos\theta&-r\sin\theta\\
\sin\theta&r\cos\theta
\end{vmatrix}=r.\]
On the other hand, if we want to work the other way then we have
\[r=\sqrt{x^2+y^2},\quad\tan\theta=\frac{y}{x}.\]
The Jacobian matrix of this transformation is given by
\[\begin{pmatrix}
\pdv{r}{x}&\pdv{r}{y}\\
\pdv{\theta}{x}&\pdv{\theta}{y}
\end{pmatrix}=
\begin{pmatrix}
\frac{x}{\sqrt{x^2+y^2}}&\frac{y}{\sqrt{x^2+y^2}}\\
-\frac{y}{x^2+y^2}&\frac{x}{x^2+y^2}
\end{pmatrix}\]
so that its Jacobian is
\[\frac{\partial(r,\theta)}{\partial(x,y)}=\frac{1}{\sqrt{x^2+y^2}}=\frac{1}{r}.\]
Note that
\[\frac{\partial(x,y)}{\partial(r,\theta)}\frac{\partial(r,\theta)}{\partial(x,y)}=1.\]
This result holds more generally, as you will see now.

\begin{proposition}
Let $r$ and $s$ be fuunctions of variables $u$ and $v$ which in turn are functions of $x$ and $y$. Then
\[\frac{\partial(r,s)}{\partial(x,y)}=\frac{\partial(r,s)}{\partial(u,v)}\frac{\partial(u,v)}{\partial(x,y)}.\]
\end{proposition}

\begin{proof}
\begin{align*}
\frac{\partial(r,s)}{\partial(x,y)}
&=\begin{vmatrix}
\pdv{r}{x}&\pdv{r}{y}\\
\pdv{s}{x}&\pdv{s}{y}
\end{vmatrix}\\
&=\begin{vmatrix}
\pdv{r}{u}\pdv{u}{x}+\pdv{r}{v}\pdv{v}{x}&\pdv{r}{u}\pdv{u}{y}+\pdv{r}{v}\pdv{v}{y}\\
\pdv{s}{u}\pdv{u}{x}+\pdv{s}{v}\pdv{v}{x}&\pdv{s}{u}\pdv{u}{y}+\pdv{s}{v}\pdv{v}{y}
\end{vmatrix}\\
&=\begin{vmatrix}
\begin{pmatrix}
\pdv{r}{u}&\pdv{r}{v}\\
\pdv{s}{u}&\pdv{s}{v}
\end{pmatrix} &
\begin{pmatrix}
\pdv{u}{x}&\pdv{u}{y}\\
\pdv{v}{x}&\pdv{v}{y}
\end{pmatrix}
\end{vmatrix}\\
&=\begin{vmatrix}
\pdv{r}{u}&\pdv{r}{v}\\
\pdv{s}{u}&\pdv{s}{v}
\end{vmatrix}
\begin{vmatrix}
\pdv{u}{x}&\pdv{u}{y}\\
\pdv{v}{x}&\pdv{v}{y}
\end{vmatrix}\\
&=\frac{\partial(r,s)}{\partial(u,v)}\frac{\partial(u,v)}{\partial(x,y)}.
\end{align*}
(The penultimate line here comes from the fact that, for any square matrices $A$ and $B$, $\mathrm{det}(AB)=\mathrm{det}(A)\mathrm{det}(B)=\mathrm{det}(BA)$.)
\end{proof}

Taking $r=x$ and $s=y$ gives us the following corollay.

\begin{corollary}
\[\frac{\partial(x,y)}{\partial(u,v)}\frac{\partial(u,v)}{\partial(x,y)}=1.\]
\end{corollary}

Indeed, the stronger result
\[\begin{pmatrix}
\pdv{x}{u}&\pdv{x}{v}\\
\pdv{y}{u}&\pdv{y}{v}
\end{pmatrix}
\begin{pmatrix}
\pdv{u}{x}&\pdv{u}{y}\\
\pdv{v}{x}&\pdv{v}{y}
\end{pmatrix}=
\begin{pmatrix}
1&0\\0&1
\end{pmatrix}
\]
also holds true. Although we do not prove this result here, we illustrate a use of it in the next exercise, before we continue with plane polar coordinates.

\begin{exercise}
Calculate $u_x$, $u_y$, $v_x$ and $v_y$ in terms of $u$ and $v$< given that
\[x=a\cosh u\cos v,\quad y=a\sinh u\sin v.\]
\end{exercise}

\begin{solution}
We have
\[\begin{pmatrix}
x_u&x_v\\
y_u&y_v
\end{pmatrix}=
\begin{pmatrix}
a\sinh u\cos v&-a\cosh u\sin v\\
a\cosh u\sin v&a\sinh u\cos v
\end{pmatrix}.\]
Hence
\begin{align*}
\begin{pmatrix}
u_x&u_y\\
v_x&v_y
\end{pmatrix}&=
\begin{pmatrix}
a\sinh u\cos v&-a\cosh u\sin v\\
a\cosh u\sin v&a\sinh u\cos v
\end{pmatrix}^{-1}\\
&=\frac{1}{a\brac{\sinh^2u\cos^2v+\cosh^2u\sin^2v}}\begin{pmatrix}
\sinh u\cos v&\cosh u\sin v\\
-\cosh u\sin v&\sinh u\cos v
\end{pmatrix}.
\end{align*}
\end{solution}

Going back to plane polar coordinates, we now make an important point about notation. The definition of the partial derivative $\partial f/\partial x$ very much depends on the coordinate system that $x$ is part of. It is important to know which other coordinates are being kept fixed. For example, we could have two different coordinate systems, one with the standard Cartesian coordinates $x$ and $y$ and the other being $x$ and the polar coordinate $\theta$. Consider now what $\partial r/\partial x$ means in each system. In Cartesian coordinates, we have
\[r=\sqrt{x^2+y^2}\text{ and so }\pdv{r}{x}=\frac{x}{\sqrt{x^2+y^2}};\]
we have held $y$ constant. However, when we write $r$ in terms of $x$ and $\theta$ then we have
\[r=\frac{x}{\cos\theta}\text{ and so }\pdv{r}{x}=\frac{1}{\cos\theta}=\frac{\sqrt{x^2+y^2}}{x};\]
here we have held $\theta$ constant.

The answers are certainly different! The reason is that the two derivatives that we have calculated are, respectively,
\[\brac{\pdv{r}{x}}_y\quad\text{and}\quad\brac{\pdv{r}{x}}_\theta\]
and so we are measuring the change in $x$ along curves $y=$ constant or along $\theta=$ constant, which are very different directions.

Note also the role of the Jacobian matrix if we want to change the direction of the transformation. For example, if $f(x y)$ is a function with continuous partial derivatives, and $x=r\cos\theta$, $y=r\sin\theta$, then we have, from the chain rule,
\begin{align*}
\pdv{f}{r}&=\cos\theta\pdv{f}{x}+\sin\theta\pdv{f}{y},\\
\pdv{f}{\theta}&=-r\sin\theta\pdv{f}{x}+r\cos\theta\pdv{f}{y},
\end{align*}
which can be written as


\subsection{Parabolic coordinates}

\subsection{Cylindrical polar coordinates}

\subsection{Spherical polar coordinates}


\pagebreak

\section{Directional Derivatives}
To this point we've only looked at the two partial derivatives $f_x(x,y)$ and $f_y(x,y)$. Recall that these derivatives represent the rate of change of $f$ as we vary x (holding $y$ fixed) and as we vary $y$ (holding $x$ fixed) respectively. 

We now discuss how to find the rate of change of $f$ if we allow both $x$ and $y$ to change simultaneously. The problem here is that there are many ways to allow both $x$ and $y$ to change. For instance, one could be changing faster than the other and then there is also the issue of whether or not each is increasing or decreasing. So, before we get into finding the rate of change we need to get a couple of preliminary ideas taken care of first. The main idea that we need to look at is just how are we going to define the changing of $x$ and/or $y$.

Let's start off by supposing that we wanted the rate of change of $f$ at a particular point, say $(x_0,y_0)$. Let's also suppose that both $x$ and $y$ are increasing and that, in this case, $x$ is increasing twice as fast as $y$ is increasing. So as $y$ increases one unit of measure, $x$ increases two units of measure.

Let's suppose that a particle is sitting at $(x_0,y_0)$ and the particle will move in the direction given by the changing 
$x$ and $y$. At this point, the particle can be said to be moving in the direction
\[ \vec{v} = \langle {2,1} \rangle \]

There is still a small problem with this however. There are many vectors that point in the same direction. For instance, all of the following vectors point in the same direction as $\vec v = \langle {2,1} \rangle$:
\[ \vec{v} = \left\langle {\frac{1}{5},\frac{1}{10}}\right\rangle \quad \vec{v} = \langle {6,3}\rangle \quad \vec{v} = \left\langle {\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}}}\right\rangle \]

We need a way to consistently find the rate of change of a function in a given direction. We will do this by insisting that the vector that defines the direction of change be a unit vector. This means that for the example that we started off thinking about we would want to use
\[ \vec{v} = \left\langle {\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}}}\right\rangle \]

\begin{definition}[Directional derivative]
Rate of change of $f(x,y)$ in the direction of the unit vector $\vec{u}=\langle{a,b}\rangle$ is called the directional derivative and is denoted by $D_{\vec{u}} f(x,y)$.

The definition of the directional derivative is
\begin{equation}
{D_{\vec u}}f(x,y) = \lim_{h\to0} \frac{f(x + ah,y + bh) - f(x,y)}{h}
\end{equation}
\end{definition}

To derive an equivalent formula for taking directional derivatives, we define a new function of a single variable
\[ g(z) = f(x_0+az,y_0+bz) \]
where $x_0$, $y_0$, $a$, $b$ are some fixed numbers. Note that this really is a function of a single variable $z$.

Then by the definition of the derivative for functions of a single variable we have
\[ g^\prime(z) = \lim_{h\to0} \frac{g(z+h)-g(z)}{h} \]
and the derivative at $z=0$ is given by
\[ g^\prime(0) = \lim_{h\to0} \frac{g(h)-g(0)}{h} \]
If we now substitute in for $g(z)$ we get
\[ g^\prime(0) = \lim_{h\to0} \frac{g(h)-g(0)}{h} = \lim_{h\to0} \frac{f(x_0+ah,y_0+bh) - f(x_0,y_0)}{h} = D_{\vec u}f(x_0,y_0) \]
This gives us
\begin{equation}\tag{1}
g^\prime(0) = D_{\vec u}f(x_0,y_0)
\end{equation}

Now, let's look at this from another perspective. Let's rewrite $g(z)$ as $g(z) = f(x,y)$ where $x=x_0+az$ and $y=y_0+bz$. Applying chain rule,
\[ g^\prime(z) = \odv{g}{z} = \pdv{f}{x}\odv{x}{z} + \pdv{f}{y}\odv{y}{z} = f_x (x,y)a + f_y (x,y)b \]
This gives us
\[ g^\prime(z) = f_x (x,y)a + f_y (x,y)b \]
If we take $z=0$ we get $x=x_0$ and $y=y_0$. Plugging these into the above equation gives
\begin{equation}\tag{2}
g^\prime(0) = f_x (x_0,y_0)a + f_y (x_0,y_0)b
\end{equation}
Equating (1) and (2) gives
\[ {D_{\vec u}}f(x_0,y_0) = f_x(x_0,y_0)a + f_y(x_0,y_0)b \]
Allowing $x$ and $y$ to be any number we get the following formula for computing directional derivatives:
\[ {D_{\vec u}}f(x,y) = f_x(x,y)a + f_y(x,y)b \]
For three variables, directional derivative of $f(x,y,z)$ in the direction of the unit vector $\vec{u}=\langle{a,b,c}\rangle$ is given by
\begin{equation}
{D_{\vec u}}f(x,y,z) = f_x (x,y,z)a + f_y (x,y,z)b + f_z (x,y,z)c
\end{equation}

We can write the directional derivative as a \textbf{dot product} and notice that the second vector is nothing more than the unit vector $\vec u$ that gives the direction of change.
\begin{equation}
{D_{\vec u}} f(x,y,z) = \langle {f_x,f_y,f_z} \rangle \cdot \langle {a,b,c} \rangle
\end{equation}

Now let's give a name and notation to the first vector in the dot product since this vector will show up fairly regularly.
\begin{definition}[Gradient vector]
The gradient vector of $f$ is defined to be
\begin{equation}
\nabla f = \langle f_x,f_y,f_z \rangle
\end{equation}
\end{definition}

With the definition of the gradient we can now say that the directional derivative is given by
\[ {D_{\vec u}}f = \nabla f\cdot \vec u \]

\begin{theorem}
Maximum value of $D_{\vec u} f(\vec{x})$ (and hence then the maximum rate of change of the function $f(\vec{x})$) is given by $\left\|\nabla f(\vec{x})\right\|$ and will occur in the direction given by $\nabla f(\vec{x})$.
\end{theorem}

\begin{proof}

\end{proof}
\pagebreak

\chapter{Partial Differential Equations}
\section{Definitions and Terminology}
\begin{definition}
A \vocab{partial differential equation} is an equation involving a function and/or its partial derivatives. 
\end{definition}

We can classify PDEs based on:
\begin{itemize}
\item \textbf{Order.}

The order is the number corresponding to the order of the highest partial derivative in the equation. 

For instance, the order of the following PDE is 2. 
\[ \pdv[order={2}]{f}{x}=\pdv{f}{t} \]

This also applies to mixed partial derivatives. For instance, the order of the following PDE is 3.
\[ \pdv[order={2,1}]{f}{x,y}=\pdv{f}{t} \]

\item \textbf{Number of independent variables.}

An independent variable is what we differentiate with respect to. 

\item \textbf{Linearity.}

A linear PDE is one in which the \emph{dependent} variable (the one being differentiated) appears only in a linear fashion.

For instance, the two PDEs above are linear as the partial derivatives are not being raised to a power or multiplied with each other.

The following PDE is non-linear.
\[ f\pdv[order={2}]{f}{x}=\pdv{f}{t} \]

\item \textbf{Homogeneity.}

A homogenous PDE is one in which every term only involves the dependent variable and/or its derivatives.

The first two PDEs above are homogenous as every term contains $f$ or its derivatives.

The following PDE is non-homogenous as there are two terms that do not contain $f$.
\[ \pdv[order={2}]{f}{x}=\pdv{f}{t}+x^2+\tan t \]

\item \textbf{Coefficient type.}

The coefficient here refers to the coefficient of the term involving the dependent variable and its derivatives. It can be either constant or variable.

For instance, the coefficients of the terms in the first two examples are 1. We say that these two PDEs have constant coefficients.

The following PDE has variable coefficients.
\[ \tan x\pdv[order={2}]{f}{x}=\pdv{f}{t} \]

\item \textbf{Parabolic, Hyperbolic, or Elliptic.}

We can do this classification for linear 2nd order PDEs which take the form of 
\[ A\pdv[order={2}]{f}{x} + B\pdv{f}{x,y} + C\pdv[order={2}]{f}{y} + D\pdv{f}{x} + E\pdv{f}{y} + Ff = G \]
where the coefficients are generally functions of $x$ or $y$.

For a \textbf{hyperbolic} PDE, $B^2-4AC>0$. Using variable substitutions to change $x$ and $y$ to $\eta$ and $\epsilon$ respectively, we can reduce the PDE to \[ \pdv[order={2}]{f}{\eta} - \pdv[order={2}]{f}{\epsilon} + g = 0 \] where $g$ denotes the first and lower order terms. This is similar to the equation of a hyperbola: $x^2-y^2=1$.

For a \textbf{parabolic} PDE, $B^2-4AC=0$. Using variable substitutions, we can reduce the PDE to \[ \pdv[order={2}]{f}{\eta} + g = 0. \] This is similar to the equation of a parabola: $x^2+y=0$.

For an \textbf{elliptic} PDE, $B^2-4AC<0$. Using variable substitutions, we can reduce the PDE to \[ \pdv[order={2}]{f}{\eta} + \pdv[order={2}]{f}{\epsilon} + g = 0. \] This is similar to the equation of an ellipse: $x^2+y^2=1$.

Note that if the coefficients are constants, the PDE can be hyperbolic, parabolic or elliptic. However, if the coefficients are variables, then it is possible for the PDE to be hyperbolic in some regions, and elliptic or parabolic in some regions.
\end{itemize}

\section{Solutions and Auxiliary Conditions}
\begin{exercise}
Show that $f(x,y)=\tan^{-1}\frac{y}{x}$ satisfies Laplace's equation in the plane:
\[ \pdv{[order={2}]}{f}{x}+\pdv{[order={2}]}{f}{y}=0. \]
\end{exercise}

\begin{solution}
The first order partial derivatives are
\[ \pdv{f}{x}=\frac{1}{1+\brac{\frac{y}{x}}^2}\brac{-\frac{y}{x^2}}=-\frac{y}{x^2+y^2} \]
and
\[ \pdv{f}{y}=\frac{1}{1+\brac{\frac{y}{x}}^2}\brac{\frac{1}{x}}=\frac{x}{x^2+y^2}. \]
Hence we have
\[ \pdv{[order={2}]}{f}{x}=\frac{2xy}{x^2+y^2} \quad \text{and} \quad \pdv{[order={2}]}{f}{y}=-\frac{2xy}{(x^2+y^2)^2}, \]
from which we see that
\[ \pdv{[order={2}]}{f}{x}+\pdv{[order={2}]}{f}{y}=0 \]
as required.
\end{solution}

In the above example we verified that a given solution satisfied a given PDE. We now look at how to find solutions to simple PDEs.

\begin{exercise}
Find all the solutions of the form $f(x,y)$ of the PDEs
\begin{enumerate}[label=(\alph*)]
\item $\pdv{f}{y,x}=0$,
\item $\pdv{[order={2}]}{f}{x}=0$.
\end{enumerate}
\end{exercise}

\begin{solution} \
\begin{enumerate}[label=(\alph*)]
\item We have
\[ \pdv{}{y}\brac{\pdv{f}{x}}=0. \]
Those functions $g(x,y)$  which satisfy $\delta g/\delta y=0$ are functions which solely depend on $x$. So we have
\[ \pdv{f}{x}=p(x), \]
where $p$ is an arbitrary function of $x$. We can now integrate again, but this time with respect to $x$ rather than $y$. Now, $\delta/\delta x$ sends to zero any function which solely depends on $y$. The solution to the PDE is therefore
\[ f(x,y)=P(x)+Q(y), \]
where $Q(y)$ is an arbitrary function of $y$ and $P(x)$ is an anti-derivative of $p(x)$, i.e. $P^\prime(x)=p(x)$.
\end{enumerate}
\end{solution}

If a PDE involves derivatives with respect to one variable only, we can treat it like an ODE in that variable, holding all other variables constant. The difference, as noted above, is that our arbitrary ``constants'' will now be arbitrary functions of the variables that we have held constant.

\begin{exercise}
Find solutions $u(x,y)$ of the PDE
\[ u_{xx}-u=0. \]
\end{exercise}

\begin{solution}
Since there are no derivatives with respect to $y$, we can solve the associated ODE
\[ \dv[2]{u}{x}-u=0, \]
where $u$ is treated as being a function of $x$ only. This ODE has solution $u(x)=C_1e^x+C_2e^{-x}$, where $C_1$ and $C_2$ are constants, and so the solution to the original PDE is
\[ u(x,y)=A(y)e^x+B(y)e^{-x}, \]
where $A$ and $B$ are arbitrary functions of $y$ only.
\end{solution}

we look at one specific method for solving PDEs, that of \textbf{separating the variables}.

\begin{exercise}
Find all solutions of the form $T(x,t)=A(x)B(t)$ to the one-dimensional heat/diffusion equation
\[ \pdv{T}{t}=\kappa\pdv{[order={2}]}{T}{x}, \]
where $\kappa$ is a positive constant, called the thermal diffusivity.
\end{exercise}

Solutions of the form $A(x)B(t)$ are known as separable solutions

There are a lot of solutions to a given PDE, hence it is important for us to know the auxiliary conditions, i.e. boundary and initial conditions, which dictate which technique we use to solve the PDE.
\begin{itemize}
\item A boundary condition expresses the behavior of a function on the boundary (border) of its area of definition. An initial condition is like a boundary condition, but then for the time-direction.
\end{itemize}
\pagebreak